# Adding a human touch to piano score
## Lou Baudoin

This repository contains the code for my bachelor thesis. The scripts in this directory are mostly used for convenience and evaluation purposes and should be edited to point to the appropriate directories for the model and data. The most important of these is [humanize.py](humanize.py), since it provides an easy way to quantise and subsequently humanise MIDI files. The included example files are from the validation split of the [MAESTRO dataset](https://magenta.tensorflow.org/datasets/maestro#v300) and were used when developing the model to evaluate the performance.

The code for the annotation tool used in the survey is in the [Survey](Survey/) directory. Note that the samples are not included. These were generated by first training the model and subsequently running the [humanize.py](humanize.py) and [synthesize.py](synthesize.py) scripts on the test split of MAESTRO.

The code in the [magenta-music_vae](magenta-music_vae/) directory are additions/adaptations to MusicVAE in Google's project [Magenta](https://magenta.tensorflow.org/) and contain the actual model architecture and data processing scripts.

## Installation

To use any of this code, you first need to set up your [Magenta development environment](https://github.com/magenta/magenta/blob/main/README.md#development-environment) and download the [MAESTRO dataset](https://magenta.tensorflow.org/datasets/maestro#v300), extract it to a subdirectory named 'data' and run the [split.py](split.py) script to organise the MIDI files properly.

Once that's done, add the scripts from [magenta-music_vae](magenta-music_vae/) to the 'magenta/magenta/models/music_vae' directory and use the commands below or refer to the [MusicVAE documentation](https://github.com/magenta/magenta/tree/main/magenta/models/music_vae) for further information on how to train a model.

## Useful commands

To convert all MIDI files in a directory to a tfrecord for training or evaluating (run [split.py](split.py) first):
`convert_dir_to_note_sequences --input_dir=data/train --output_file=data/processed/train.tfrecord --recursive`
`convert_dir_to_note_sequences --input_dir=data/validation --output_file=data/processed/validation.tfrecord --recursive`

To train the model:
`music_vae_train --config=lbau_2bar --run_dir=models/lbau_2bar/ --mode=train`
And run evaluation (concurrently):
`music_vae_train --config=lbau_2bar_eval --run_dir=models/lbau_2bar/ --mode=eval`
To keep track of training and evaluation, it's a good idea to launch tensorboard:
`tensorboard --logdir models/lbau_2bar_big/train`